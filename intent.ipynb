{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente iremos realizar a leitura dos dados que serão utilizados para pergunta e resposta. Estamos considerando como other a intenção de Q&A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other</td>\n",
       "      <td>What is the principle behind flight?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other</td>\n",
       "      <td>What are the four forces acting on an airplane?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other</td>\n",
       "      <td>What is the difference between IFR and VFR?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "      <td>What is a black box in aviation?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other</td>\n",
       "      <td>What is the busiest airport in the world by pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>other</td>\n",
       "      <td>What is \"ACARS\"?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>other</td>\n",
       "      <td>What is \"Alternate Airport\"?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>other</td>\n",
       "      <td>at does \"pan-pan\" mean?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>other</td>\n",
       "      <td>What is \"decision height\" (DH)?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>other</td>\n",
       "      <td>What is \"angle of incidence\"?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0   other               What is the principle behind flight?\n",
       "1   other    What are the four forces acting on an airplane?\n",
       "2   other        What is the difference between IFR and VFR?\n",
       "3   other                   What is a black box in aviation?\n",
       "4   other  What is the busiest airport in the world by pa...\n",
       "..    ...                                                ...\n",
       "94  other                                   What is \"ACARS\"?\n",
       "95  other                       What is \"Alternate Airport\"?\n",
       "96  other                            at does \"pan-pan\" mean?\n",
       "97  other                    What is \"decision height\" (DH)?\n",
       "98  other                      What is \"angle of incidence\"?\n",
       "\n",
       "[99 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_data = pd.read_csv(\"q&a_intent_train.csv\", names= [\"target\", \"text\"])\n",
    "\n",
    "qa_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leitura dos dados de treino e teste disponibilizados pelo ATIS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATIS train dataset size is: 4834\n",
      "ATIS test dataset size is: 800\n"
     ]
    }
   ],
   "source": [
    "atis_train_data = pd.read_csv(\"atis_intents_train.csv\", names= [\"target\", \"text\"])\n",
    "atis_test_data = pd.read_csv(\"atis_intents_test.csv\", names= [\"target\", \"text\"])\n",
    "\n",
    "print(\"ATIS train dataset size is:\", len(atis_train_data))\n",
    "print(\"ATIS test dataset size is:\", len(atis_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos construir um dataset para treino e teste considerando os dados que temos até então. Para tanto, separaremos qa_data em treino e teste (considerando por volta de 20% para teste) e depois construíremos um dataset para treino e um para teste unindo as tabelas até então existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A train dataset size is: 79\n",
      "Q&A test dataset size is: 20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "qa_train_data, qa_test_data = train_test_split(qa_data, test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"Q&A train dataset size is:\", len(qa_train_data))\n",
    "print(\"Q&A test dataset size is:\", len(qa_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other</td>\n",
       "      <td>What is a \"taxiway\"?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other</td>\n",
       "      <td>What is a \"slot-restricted\" airport?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other</td>\n",
       "      <td>What is \"NextGen\" in U.S. aviation?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "      <td>What does the term \"gate hold\" mean in aviation?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other</td>\n",
       "      <td>What does \"direct flight\" mean as opposed to \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4908</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>what is the airfare for flights from denver t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4909</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>do you have any flights from denver to baltim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4910</th>\n",
       "      <td>atis_airline</td>\n",
       "      <td>which airlines fly into and out of denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4911</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>does continental fly from boston to san franc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4912</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>is there a delta flight from denver to san fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4913 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            target                                               text\n",
       "0            other                               What is a \"taxiway\"?\n",
       "1            other               What is a \"slot-restricted\" airport?\n",
       "2            other                What is \"NextGen\" in U.S. aviation?\n",
       "3            other   What does the term \"gate hold\" mean in aviation?\n",
       "4            other  What does \"direct flight\" mean as opposed to \"...\n",
       "...            ...                                                ...\n",
       "4908  atis_airfare   what is the airfare for flights from denver t...\n",
       "4909   atis_flight   do you have any flights from denver to baltim...\n",
       "4910  atis_airline          which airlines fly into and out of denver\n",
       "4911   atis_flight   does continental fly from boston to san franc...\n",
       "4912   atis_flight   is there a delta flight from denver to san fr...\n",
       "\n",
       "[4913 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.concat([qa_train_data, atis_train_data], ignore_index=True)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other</td>\n",
       "      <td>What is \"yaw\" in aviation?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other</td>\n",
       "      <td>What is a \"deadhead\" flight?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other</td>\n",
       "      <td>What is \"Alternate Airport\"?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "      <td>What is the purpose of ailerons on an aircraft?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other</td>\n",
       "      <td>What is \"decision height\" (DH)?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>please find all the flights from cincinnati t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>find me a flight from cincinnati to any airpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i'd like to fly from miami to chicago on amer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i would like to book a round trip flight from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>find me a flight that flies from memphis to t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>820 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          target                                               text\n",
       "0          other                         What is \"yaw\" in aviation?\n",
       "1          other                       What is a \"deadhead\" flight?\n",
       "2          other                       What is \"Alternate Airport\"?\n",
       "3          other    What is the purpose of ailerons on an aircraft?\n",
       "4          other                    What is \"decision height\" (DH)?\n",
       "..           ...                                                ...\n",
       "815  atis_flight   please find all the flights from cincinnati t...\n",
       "816  atis_flight   find me a flight from cincinnati to any airpo...\n",
       "817  atis_flight   i'd like to fly from miami to chicago on amer...\n",
       "818  atis_flight   i would like to book a round trip flight from...\n",
       "819  atis_flight   find me a flight that flies from memphis to t...\n",
       "\n",
       "[820 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.concat([qa_test_data, atis_test_data], ignore_index=True)\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target One Hot Enconding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "\n",
    "y_encoder= OHE().fit(np.array(train_data.target).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr_encoded= y_encoder.transform(np.array(train_data.target).reshape(-1,1)).toarray()\n",
    "yts_encoded= y_encoder.transform(np.array(test_data.target).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing With NLTK and TensorFlor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/wiltonramos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wiltonramos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"lower_text\"]= train_data.text.map(lambda x: x.lower())\n",
    "test_data[\"lower_text\"]= test_data.text.map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "train_data[\"tokenized\"]= train_data.lower_text.map(word_tokenize)\n",
    "test_data[\"tokenized\"]= test_data.lower_text.map(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "def remove_stop(strings, stop_list):\n",
    "    classed= [s for s in strings if s not in stop_list]\n",
    "    return classed\n",
    "\n",
    "stop= stopwords.words(\"english\")\n",
    "stop_punc= list(set(punctuation))+ stop\n",
    "\n",
    "train_data[\"selected\"]= train_data.tokenized.map(lambda df: remove_stop(df, stop_punc))\n",
    "test_data[\"selected\"]= test_data.tokenized.map(lambda df: remove_stop(df, stop_punc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def normalize(text):\n",
    "    return \" \".join(text)\n",
    "\n",
    "stemmer= PorterStemmer()\n",
    "\n",
    "train_data[\"stemmed\"]= train_data.selected.map(lambda xs: [stemmer.stem(x) for x in xs])\n",
    "train_data[\"normalized\"]= train_data.stemmed.apply(normalize)\n",
    "\n",
    "test_data[\"stemmed\"]= test_data.selected.map(lambda xs: [stemmer.stem(x) for x in xs])\n",
    "test_data[\"normalized\"]= test_data.stemmed.apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer= Tokenizer(num_words= 10000)\n",
    "tokenizer.fit_on_texts(train_data.normalized)\n",
    "\n",
    "tokenized_train= tokenizer.texts_to_sequences(train_data.normalized)\n",
    "tokenized_test= tokenizer.texts_to_sequences(test_data.normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "778"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index.keys().__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_padded= pad_sequences(tokenized_train, maxlen= 20, padding= \"pre\")\n",
    "test_padded= pad_sequences(tokenized_test, maxlen= 20, padding= \"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4913, 20)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X Matrix (samples, steps, wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function transform final processed text (columns padded) into 3D matrix (samples, steps, unique_words)\n",
    "#matrix contents one hot encoded words. Encoding was done for each step and based on unique words\n",
    "\n",
    "def transform_x(data, tokenizer):\n",
    "    output_shape= [data.shape[0],\n",
    "                  data.shape[1],\n",
    "                  tokenizer.word_index.keys().__len__()]\n",
    "    results= np.zeros(output_shape)\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        for ii in range(data.shape[1]):\n",
    "            results[i, ii, data[i,ii]-1]= 1\n",
    "    return results\n",
    "\n",
    "xtr_transformed= transform_x(train_padded, tokenizer)\n",
    "xts_transformed= transform_x(test_padded, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, BatchNormalization, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy as CC\n",
    "from tensorflow.keras.activations import relu, softmax\n",
    "from tensorflow.keras.initializers import he_uniform, glorot_uniform\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "class LSTMModel(object):\n",
    "    \n",
    "    def build_model(self, input_dim, output_shape, steps, dropout_rate, kernel_regularizer, bias_regularizer):\n",
    "        input_layer= Input(shape= (steps, input_dim))\n",
    "        \n",
    "        #make lstm_layer\n",
    "        lstm= LSTM(units= steps)(input_layer)\n",
    "        dense_1= Dense(output_shape, kernel_initializer= he_uniform(),\n",
    "                       bias_initializer= \"zeros\", \n",
    "                       kernel_regularizer= l2(l= kernel_regularizer),\n",
    "                       bias_regularizer= l2(l= bias_regularizer))(lstm)\n",
    "        x= BatchNormalization()(dense_1)\n",
    "        x= relu(x)\n",
    "        x= Dropout(rate= dropout_rate)(x)\n",
    "        o= Dense(output_shape, kernel_initializer= glorot_uniform(),\n",
    "                 bias_initializer= \"zeros\", \n",
    "                 kernel_regularizer= l2(l= kernel_regularizer), \n",
    "                 bias_regularizer= l2(l= bias_regularizer))(dense_1)\n",
    "        o= BatchNormalization()(o)\n",
    "        output= softmax(o, axis= 1)\n",
    "        \n",
    "        loss= CC()\n",
    "        metrics= AUC()\n",
    "        optimizer= Adam()\n",
    "        self.model= Model(inputs= [input_layer], outputs= [output])\n",
    "        self.model.compile(optimizer= optimizer, loss= loss, metrics= [metrics])\n",
    "        \n",
    "        \n",
    "    def train(self, x, y, validation_split, epochs):\n",
    "        self.model.fit(x, y, validation_split= validation_split, epochs= epochs)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "steps= xtr_transformed.shape[1]\n",
    "dim= xtr_transformed.shape[2]\n",
    "output_shape= ytr_encoded.shape[1]\n",
    "\n",
    "model= LSTMModel()\n",
    "model.build_model(input_dim= dim,\n",
    "                  output_shape= output_shape,\n",
    "                  steps= steps, \n",
    "                  dropout_rate= 0.5, \n",
    "                  bias_regularizer= 0.3, \n",
    "                  kernel_regularizer= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "123/123 [==============================] - 2s 9ms/step - loss: 7.7637 - auc_1: 0.7616 - val_loss: 6.1412 - val_auc_1: 0.8308\n",
      "Epoch 2/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 4.2653 - auc_1: 0.9490 - val_loss: 3.6263 - val_auc_1: 0.9768\n",
      "Epoch 3/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 2.5348 - auc_1: 0.9765 - val_loss: 2.2981 - val_auc_1: 0.9745\n",
      "Epoch 4/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 1.6594 - auc_1: 0.9868 - val_loss: 1.5815 - val_auc_1: 0.9780\n",
      "Epoch 5/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 1.2275 - auc_1: 0.9903 - val_loss: 0.9953 - val_auc_1: 0.9929\n",
      "Epoch 6/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.9984 - auc_1: 0.9928 - val_loss: 0.8271 - val_auc_1: 0.9919\n",
      "Epoch 7/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.8452 - auc_1: 0.9944 - val_loss: 0.6922 - val_auc_1: 0.9968\n",
      "Epoch 8/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.7566 - auc_1: 0.9960 - val_loss: 0.5710 - val_auc_1: 0.9956\n",
      "Epoch 9/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.6833 - auc_1: 0.9965 - val_loss: 0.6108 - val_auc_1: 0.9944\n",
      "Epoch 10/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.6206 - auc_1: 0.9970 - val_loss: 0.6607 - val_auc_1: 0.9913\n",
      "Epoch 11/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.5767 - auc_1: 0.9977 - val_loss: 0.5287 - val_auc_1: 0.9961\n",
      "Epoch 12/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.5258 - auc_1: 0.9984 - val_loss: 0.4025 - val_auc_1: 0.9968\n",
      "Epoch 13/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.4910 - auc_1: 0.9981 - val_loss: 0.4478 - val_auc_1: 0.9964\n",
      "Epoch 14/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.4555 - auc_1: 0.9983 - val_loss: 0.4048 - val_auc_1: 0.9963\n",
      "Epoch 15/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.4321 - auc_1: 0.9990 - val_loss: 0.3840 - val_auc_1: 0.9964\n",
      "Epoch 16/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.4052 - auc_1: 0.9986 - val_loss: 0.3420 - val_auc_1: 0.9965\n",
      "Epoch 17/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.3870 - auc_1: 0.9990 - val_loss: 0.3741 - val_auc_1: 0.9983\n",
      "Epoch 18/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.3641 - auc_1: 0.9993 - val_loss: 0.3098 - val_auc_1: 0.9971\n",
      "Epoch 19/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.3507 - auc_1: 0.9992 - val_loss: 0.3243 - val_auc_1: 0.9960\n",
      "Epoch 20/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.3322 - auc_1: 0.9993 - val_loss: 0.3314 - val_auc_1: 0.9971\n",
      "Epoch 21/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.3154 - auc_1: 0.9995 - val_loss: 0.2909 - val_auc_1: 0.9968\n",
      "Epoch 22/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.3024 - auc_1: 0.9996 - val_loss: 0.2757 - val_auc_1: 0.9974\n",
      "Epoch 23/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.2898 - auc_1: 0.9995 - val_loss: 0.2857 - val_auc_1: 0.9958\n",
      "Epoch 24/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.2846 - auc_1: 0.9995 - val_loss: 0.2689 - val_auc_1: 0.9970\n",
      "Epoch 25/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.2684 - auc_1: 0.9996 - val_loss: 0.2639 - val_auc_1: 0.9968\n",
      "Epoch 26/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.2609 - auc_1: 0.9995 - val_loss: 0.2415 - val_auc_1: 0.9976\n",
      "Epoch 27/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.2489 - auc_1: 0.9997 - val_loss: 0.2450 - val_auc_1: 0.9971\n",
      "Epoch 28/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.2441 - auc_1: 0.9997 - val_loss: 0.2369 - val_auc_1: 0.9972\n",
      "Epoch 29/60\n",
      "123/123 [==============================] - 1s 8ms/step - loss: 0.2306 - auc_1: 0.9996 - val_loss: 0.2190 - val_auc_1: 0.9983\n",
      "Epoch 30/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.2256 - auc_1: 0.9998 - val_loss: 0.2218 - val_auc_1: 0.9978\n",
      "Epoch 31/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.2179 - auc_1: 0.9997 - val_loss: 0.2266 - val_auc_1: 0.9976\n",
      "Epoch 32/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.2085 - auc_1: 0.9998 - val_loss: 0.2105 - val_auc_1: 0.9984\n",
      "Epoch 33/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.2072 - auc_1: 0.9998 - val_loss: 0.2050 - val_auc_1: 0.9979\n",
      "Epoch 34/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1991 - auc_1: 0.9997 - val_loss: 0.2053 - val_auc_1: 0.9984\n",
      "Epoch 35/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1949 - auc_1: 0.9998 - val_loss: 0.2079 - val_auc_1: 0.9978\n",
      "Epoch 36/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1858 - auc_1: 0.9998 - val_loss: 0.2073 - val_auc_1: 0.9979\n",
      "Epoch 37/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1845 - auc_1: 0.9998 - val_loss: 0.1988 - val_auc_1: 0.9985\n",
      "Epoch 38/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1759 - auc_1: 0.9997 - val_loss: 0.1925 - val_auc_1: 0.9986\n",
      "Epoch 39/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1700 - auc_1: 0.9998 - val_loss: 0.1861 - val_auc_1: 0.9987\n",
      "Epoch 40/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1668 - auc_1: 0.9999 - val_loss: 0.1829 - val_auc_1: 0.9988\n",
      "Epoch 41/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.1605 - auc_1: 0.9999 - val_loss: 0.1827 - val_auc_1: 0.9986\n",
      "Epoch 42/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.1596 - auc_1: 0.9998 - val_loss: 0.1798 - val_auc_1: 0.9982\n",
      "Epoch 43/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1563 - auc_1: 0.9999 - val_loss: 0.1880 - val_auc_1: 0.9981\n",
      "Epoch 44/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1529 - auc_1: 0.9997 - val_loss: 0.1776 - val_auc_1: 0.9983\n",
      "Epoch 45/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1471 - auc_1: 0.9999 - val_loss: 0.1760 - val_auc_1: 0.9987\n",
      "Epoch 46/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1445 - auc_1: 0.9997 - val_loss: 0.1773 - val_auc_1: 0.9982\n",
      "Epoch 47/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.1426 - auc_1: 0.9999 - val_loss: 0.1651 - val_auc_1: 0.9984\n",
      "Epoch 48/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1362 - auc_1: 0.9998 - val_loss: 0.1648 - val_auc_1: 0.9987\n",
      "Epoch 49/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1382 - auc_1: 0.9999 - val_loss: 0.1721 - val_auc_1: 0.9986\n",
      "Epoch 50/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1329 - auc_1: 0.9999 - val_loss: 0.1609 - val_auc_1: 0.9982\n",
      "Epoch 51/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.1308 - auc_1: 0.9999 - val_loss: 0.1557 - val_auc_1: 0.9984\n",
      "Epoch 52/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1244 - auc_1: 0.9998 - val_loss: 0.1573 - val_auc_1: 0.9984\n",
      "Epoch 53/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1259 - auc_1: 0.9998 - val_loss: 0.1550 - val_auc_1: 0.9988\n",
      "Epoch 54/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1208 - auc_1: 0.9998 - val_loss: 0.1527 - val_auc_1: 0.9988\n",
      "Epoch 55/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1252 - auc_1: 0.9999 - val_loss: 0.1564 - val_auc_1: 0.9974\n",
      "Epoch 56/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1177 - auc_1: 1.0000 - val_loss: 0.1496 - val_auc_1: 0.9979\n",
      "Epoch 57/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.1184 - auc_1: 0.9999 - val_loss: 0.1533 - val_auc_1: 0.9978\n",
      "Epoch 58/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.1162 - auc_1: 0.9999 - val_loss: 0.1564 - val_auc_1: 0.9974\n",
      "Epoch 59/60\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.1138 - auc_1: 1.0000 - val_loss: 0.1502 - val_auc_1: 0.9975\n",
      "Epoch 60/60\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.1119 - auc_1: 0.9998 - val_loss: 0.1433 - val_auc_1: 0.9975\n"
     ]
    }
   ],
   "source": [
    "model.train(xtr_transformed, ytr_encoded,\n",
    "           0.2, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 2ms/step\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "  atis_abbreviation       0.99      0.99      0.99       147\n",
      "      atis_aircraft       0.98      0.99      0.98        81\n",
      "       atis_airfare       1.00      0.99      0.99       423\n",
      "       atis_airline       0.98      0.99      0.98       157\n",
      "        atis_flight       1.00      1.00      1.00      3666\n",
      "   atis_flight_time       0.96      0.94      0.95        54\n",
      "atis_ground_service       1.00      1.00      1.00       255\n",
      "      atis_quantity       0.98      1.00      0.99        51\n",
      "              other       0.99      1.00      0.99        79\n",
      "\n",
      "           accuracy                           1.00      4913\n",
      "          macro avg       0.99      0.99      0.99      4913\n",
      "       weighted avg       1.00      1.00      1.00      4913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "prediction= y_encoder.inverse_transform(model.predict(xtr_transformed))\n",
    "print(classification_report(train_data.target, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 3ms/step\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "  atis_abbreviation       0.94      1.00      0.97        33\n",
      "      atis_aircraft       0.90      1.00      0.95         9\n",
      "       atis_airfare       1.00      0.92      0.96        48\n",
      "       atis_airline       0.90      1.00      0.95        38\n",
      "        atis_flight       0.99      0.99      0.99       632\n",
      "   atis_flight_time       1.00      1.00      1.00         1\n",
      "atis_ground_service       0.97      0.97      0.97        36\n",
      "      atis_quantity       0.43      1.00      0.60         3\n",
      "              other       1.00      0.65      0.79        20\n",
      "\n",
      "           accuracy                           0.97       820\n",
      "          macro avg       0.90      0.95      0.91       820\n",
      "       weighted avg       0.98      0.97      0.97       820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "prediction_test= y_encoder.inverse_transform(model.predict(xts_transformed))\n",
    "print(classification_report(test_data.target, prediction_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
